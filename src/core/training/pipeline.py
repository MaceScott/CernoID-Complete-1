"""
Model training pipeline.
"""
from typing import Dict, Any, Optional
import torch
from pathlib import Path
import json
import asyncio
import torch.nn as nn
from torchvision import models

from ...utils.config import get_settings
from ...utils.logging import get_logger
from ..database.service import DatabaseService

class TrainingPipeline:
    """Neural network training pipeline"""
    
    def __init__(self):
        self.settings = get_settings()
        self.logger = get_logger(__name__)
        self.db = DatabaseService()
        
        self.model = self._initialize_model()
        self.optimizer = self._initialize_optimizer()
        
    def _initialize_model(self):
        try:
            model = models.resnet50(pretrained=True)
            model.fc = nn.Sequential(
                nn.Linear(2048, self.settings.embedding_size),
                nn.BatchNorm1d(self.settings.embedding_size),
                nn.ReLU(),
                nn.Linear(self.settings.embedding_size, self.settings.num_classes)
            )
            if torch.cuda.is_available():
                model = model.cuda()
            self.logger.info("Model initialized successfully.")
            return model
        except Exception as e:
            self.logger.error(f"Model initialization failed: {str(e)}")
            raise

    def _initialize_optimizer(self):
        try:
            optimizer = torch.optim.Adam(
                self.model.parameters(),
                lr=self.settings.learning_rate,
                weight_decay=self.settings.weight_decay
            )
            self.logger.info("Optimizer initialized successfully.")
            return optimizer
        except Exception as e:
            self.logger.error(f"Optimizer initialization failed: {str(e)}")
            raise

    def train(self,
             train_data: FaceDataset,
             val_data: FaceDataset,
             epochs: int = 10) -> Dict[str, List[float]]:
        """Train the model with monitoring and validation."""
        try:
            history = {
                "train_loss": [],
                "val_loss": [],
                "accuracy": []
            }
            
            train_loader = DataLoader(
                train_data,
                batch_size=self.settings.batch_size,
                shuffle=True,
                num_workers=4
            )
            
            val_loader = DataLoader(
                val_data,
                batch_size=self.settings.batch_size,
                shuffle=False,
                num_workers=4
            )
            
            with mlflow.start_run():
                # Log parameters
                mlflow.log_params({
                    "learning_rate": self.settings.learning_rate,
                    "batch_size": self.settings.batch_size,
                    "epochs": epochs
                })
                
                for epoch in range(epochs):
                    # Training phase
                    train_loss = self._train_epoch(train_loader)
                    history["train_loss"].append(train_loss)
                    
                    # Validation phase
                    val_loss, accuracy = self._validate(val_loader)
                    history["val_loss"].append(val_loss)
                    history["accuracy"].append(accuracy)
                    
                    # Log metrics
                    mlflow.log_metrics({
                        "train_loss": train_loss,
                        "val_loss": val_loss,
                        "accuracy": accuracy
                    }, step=epoch)
                    
                    self.logger.info(
                        f"Epoch {epoch+1}/{epochs} - "
                        f"Train Loss: {train_loss:.4f} - "
                        f"Val Loss: {val_loss:.4f} - "
                        f"Accuracy: {accuracy:.4f}"
                    )
                    
                    # Save checkpoint
                    self._save_checkpoint(epoch, train_loss, val_loss)
                    
            return history
            
        except Exception as e:
            self.logger.error(f"Training error: {str(e)}")
            raise
            
    def _train_epoch(self, dataloader: DataLoader) -> float:
        """Train for one epoch."""
        self.model.train()
        total_loss = 0
        
        for batch in tqdm(dataloader, desc="Training"):
            # Get batch data
            anchors, positives, negatives = batch
            anchors = anchors.to(self.device)
            positives = positives.to(self.device)
            negatives = negatives.to(self.device)
            
            # Forward pass
            anchor_embeddings = self.model(anchors)
            positive_embeddings = self.model(positives)
            negative_embeddings = self.model(negatives)
            
            # Calculate loss
            loss = self.criterion(
                anchor_embeddings,
                positive_embeddings,
                negative_embeddings
            )
            
            # Backward pass
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            
        return total_loss / len(dataloader)
        
    def _validate(self,
                 dataloader: DataLoader) -> Tuple[float, float]:
        """Validate model performance."""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch in tqdm(dataloader, desc="Validation"):
                anchors, positives, negatives = batch
                anchors = anchors.to(self.device)
                positives = positives.to(self.device)
                negatives = negatives.to(self.device)
                
                # Get embeddings
                anchor_embeddings = self.model(anchors)
                positive_embeddings = self.model(positives)
                negative_embeddings = self.model(negatives)
                
                # Calculate loss
                loss = self.criterion(
                    anchor_embeddings,
                    positive_embeddings,
                    negative_embeddings
                )
                
                total_loss += loss.item()
                
                # Calculate accuracy
                pos_dist = torch.pairwise_distance(
                    anchor_embeddings,
                    positive_embeddings
                )
                neg_dist = torch.pairwise_distance(
                    anchor_embeddings,
                    negative_embeddings
                )
                
                correct += torch.sum(pos_dist < neg_dist).item()
                total += anchors.size(0)
                
        return (
            total_loss / len(dataloader),
            correct / total
        )
        
    def _save_checkpoint(self,
                        epoch: int,
                        train_loss: float,
                        val_loss: float):
        """Save model checkpoint."""
        try:
            checkpoint_path = Path(
                self.settings.checkpoint_dir
            ) / f"checkpoint_{epoch}.pt"

            torch.save({
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss
            }, checkpoint_path)

            mlflow.log_artifact(str(checkpoint_path))
            self.logger.info(f"Checkpoint saved successfully at epoch {epoch}: {checkpoint_path}")

        except Exception as e:
            self.logger.error(f"Failed to save checkpoint at epoch {epoch}: {str(e)}")
            raise 